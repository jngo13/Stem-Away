{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT search engine",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYYMcLgNa2cy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "b8270578-309b-4608-92da-c621a8c292d9"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip\n",
        "!pip install bert-serving-server --no-deps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-22 20:40:38--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 108.177.127.128, 172.217.218.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   100MB/s    in 3.9s    \n",
            "\n",
            "2020-08-22 20:40:42 (100 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "Collecting bert-serving-server\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.0MB/s \n",
            "\u001b[?25hInstalling collected packages: bert-serving-server\n",
            "Successfully installed bert-serving-server-1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvvFoXCR1zeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "outputId": "389ec8bb-5224-4002-f923-346f4ae5f8ba"
      },
      "source": [
        "pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.31.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=f490f1968cc6cf5f36c7aa88c0f88204c5fc242fa8f0ae71d45cd14343d44e4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLJUBvG72_wo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "9883ebab-45d2-4e6e-bd43-896d0dc01a80"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "sesh = tf.InteractiveSession()\n",
        "\n",
        "from bert_serving.server.graph import optimize_graph\n",
        "from bert_serving.server.helper import get_args_parser\n",
        "\n",
        "# input dir\n",
        "MODEL_DIR = '/content/uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "# output dir\n",
        "GRAPH_DIR = '/content/graph/' #@param {type:\"string\"}\n",
        "# output filename\n",
        "GRAPH_OUT = 'extractor.pbtxt' #@param {type:\"string\"}\n",
        "\n",
        "POOL_STRAT = 'REDUCE_MEAN' #@param ['REDUCE_MEAN', 'REDUCE_MAX', \"NONE\"]\n",
        "POOL_LAYER = '-2' #@param {type:\"string\"}\n",
        "SEQ_LEN = '256' #@param {type:\"string\"}\n",
        "\n",
        "tf.gfile.MkDir(GRAPH_DIR)\n",
        "\n",
        "parser = get_args_parser()\n",
        "carg = parser.parse_args(args=['-model_dir', MODEL_DIR,\n",
        "                               '-graph_tmp_dir', GRAPH_DIR,\n",
        "                               '-max_seq_len', str(SEQ_LEN),\n",
        "                               '-pooling_layer', str(POOL_LAYER),\n",
        "                               '-pooling_strategy', POOL_STRAT])\n",
        "\n",
        "tmp_name, config = optimize_graph(carg)\n",
        "graph_fout = os.path.join(GRAPH_DIR, GRAPH_OUT)\n",
        "\n",
        "tf.gfile.Rename(\n",
        "    tmp_name,\n",
        "    graph_fout,\n",
        "    overwrite=True\n",
        ")\n",
        "print(\"\\nSerialized graph to {}\".format(graph_fout))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 53]:model config: /content/uncased_L-12_H-768_A-12/bert_config.json\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 56]:checkpoint: /content/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt: 60]:build graph...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:132]:load parameters from checkpoint...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:136]:optimize...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:144]:freeze...\n",
            "I:\u001b[36mGRAPHOPT\u001b[0m:[gra:opt:149]:write graph to a tmp file: /content/graph/tmp1l_5jnir\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Serialized graph to /content/graph/extractor.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmgCepvNsJv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.estimator.estimator import Estimator\n",
        "from tensorflow.python.estimator.run_config import RunConfig\n",
        "from tensorflow.python.estimator.model_fn import EstimatorSpec\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "from bert_serving.server.bert.tokenization import FullTokenizer\n",
        "from bert_serving.server.bert.extract_features import convert_lst_to_features\n",
        "\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "log.handlers = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIEZPsu-kOzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GRAPH_PATH = \"/content/graph/extractor.pbtxt\" #@param {type:\"string\"}\n",
        "VOCAB_PATH = \"/content/uncased_L-12_H-768_A-12/vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "SEQ_LEN = 256 #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nULWk9-fkO5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_NAMES = ['input_ids', 'input_mask', 'input_type_ids']\n",
        "bert_tokenizer = FullTokenizer(VOCAB_PATH)\n",
        "\n",
        "def build_feed_dict(texts):\n",
        "    \n",
        "    text_features = list(convert_lst_to_features(\n",
        "        texts, SEQ_LEN, SEQ_LEN, \n",
        "        bert_tokenizer, log, False, False))\n",
        "\n",
        "    target_shape = (len(texts), -1)\n",
        "\n",
        "    feed_dict = {}\n",
        "    for iname in INPUT_NAMES:\n",
        "        features_i = np.array([getattr(f, iname) for f in text_features])\n",
        "        features_i = features_i.reshape(target_shape).astype(\"int32\")\n",
        "        feed_dict[iname] = features_i\n",
        "\n",
        "    return feed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljud_8WXIMeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_input_fn(container):\n",
        "    \n",
        "    def gen():\n",
        "        while True:\n",
        "          try:\n",
        "            yield build_feed_dict(container.get())\n",
        "          except:\n",
        "            yield build_feed_dict(container.get())\n",
        "\n",
        "    def input_fn():\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            output_types={iname: tf.int32 for iname in INPUT_NAMES},\n",
        "            output_shapes={iname: (None, None) for iname in INPUT_NAMES})\n",
        "    return input_fn\n",
        "\n",
        "class DataContainer:\n",
        "  def __init__(self):\n",
        "    self._texts = None\n",
        "  \n",
        "  def set(self, texts):\n",
        "    if type(texts) is str:\n",
        "      texts = [texts]\n",
        "    self._texts = texts\n",
        "    \n",
        "  def get(self):\n",
        "    return self._texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr4gQqBKS1rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ff3a67d-1c84-442b-f03c-408f91e1f2a0"
      },
      "source": [
        "def model_fn(features, mode):\n",
        "    with tf.gfile.GFile(GRAPH_PATH, 'rb') as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "        \n",
        "    output = tf.import_graph_def(graph_def,\n",
        "                                 input_map={k + ':0': features[k] for k in INPUT_NAMES},\n",
        "                                 return_elements=['final_encodes:0'])\n",
        "\n",
        "    return EstimatorSpec(mode=mode, predictions={'output': output[0]})\n",
        "  \n",
        "estimator = Estimator(model_fn=model_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using temporary folder as model directory: /tmp/tmp6ofyuc2v\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37oYNYf6MX1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]\n",
        "\n",
        "def build_vectorizer(_estimator, _input_fn_builder, batch_size=128):\n",
        "  container = DataContainer()\n",
        "  predict_fn = _estimator.predict(_input_fn_builder(container), yield_single_examples=False)\n",
        "  \n",
        "  def vectorize(text, verbose=False):\n",
        "    x = []\n",
        "    bar = Progbar(len(text))\n",
        "    for text_batch in batch(text, batch_size):\n",
        "      container.set(text_batch)\n",
        "      x.append(next(predict_fn)['output'])\n",
        "      if verbose:\n",
        "        bar.add(len(text_batch))\n",
        "      \n",
        "    r = np.vstack(x)\n",
        "    return r\n",
        "  \n",
        "  return vectorize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqTg_labMb3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "d03b5e17-5fcc-433a-d455-155825b7091d"
      },
      "source": [
        "bert_vectorizer = build_vectorizer(estimator, build_input_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <generator object gen at 0x7f0b8367fba0>\n",
            "RuntimeError: generator ignored GeneratorExit\n",
            "Exception ignored in: <generator object Estimator.predict at 0x7f0b843949e8>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 650, in predict\n",
            "    for key, value in six.iteritems(preds_evaluated)\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 5295, in get_controller\n",
            "    type(default))\n",
            "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etpTDXf-MX5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b68eab17-4f43-4ae1-b730-54ca3a31d242"
      },
      "source": [
        "bert_vectorizer(64*['sample text']).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoYOSce16iBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "268fd27d-7a28-4ab6-c968-144b104774da"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "max_samples = 256\n",
        "categories = ['wheat', 'tea', 'strategic-metal', \n",
        "              'housing', 'money-supply', 'fuel']\n",
        "\n",
        "S, X, Y = [], [], []\n",
        "\n",
        "for category in categories:\n",
        "  print(category)\n",
        "  \n",
        "  sents = reuters.sents(categories=category)\n",
        "  sents = [' '.join(sent) for sent in sents][:max_samples]\n",
        "  X.append(bert_vectorizer(sents, verbose=True))\n",
        "  Y += [category] * len(sents)\n",
        "  S += sents\n",
        "  \n",
        "X = np.vstack(X) \n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "wheat\n",
            "256/256 [==============================] - 4s 15ms/step\n",
            "tea\n",
            "154/154 [==============================] - 2s 15ms/step\n",
            "strategic-metal\n",
            "200/200 [==============================] - 3s 15ms/step\n",
            "housing\n",
            "139/139 [==============================] - 2s 15ms/step\n",
            "money-supply\n",
            "256/256 [==============================] - 4s 15ms/step\n",
            "fuel\n",
            "129/129 [==============================] - 2s 15ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1134, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17mZWfpV_RBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE9vB5fIuD3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "60568293-53db-4bfb-fa9e-450a284f7c63"
      },
      "source": [
        "Xtr, Xts, Ytr, Yts = train_test_split(X, Y, random_state=34)\n",
        "\n",
        "mlp = LogisticRegression()\n",
        "mlp.fit(Xtr, Ytr)\n",
        "\n",
        "print(classification_report(Yts, mlp.predict(Xts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "           fuel       0.75      0.81      0.78        26\n",
            "        housing       0.73      0.75      0.74        32\n",
            "   money-supply       0.84      0.88      0.86        75\n",
            "strategic-metal       0.88      0.90      0.89        48\n",
            "            tea       0.85      0.80      0.82        44\n",
            "          wheat       0.94      0.86      0.90        59\n",
            "\n",
            "       accuracy                           0.85       284\n",
            "      macro avg       0.83      0.83      0.83       284\n",
            "   weighted avg       0.85      0.85      0.85       284\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6JCVwa8IhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "56f57c8c-ccce-4980-d6d6-a0c3e7cfdd69"
      },
      "source": [
        "graph = tf.Graph()\n",
        "sess = tf.InteractiveSession(graph=graph)\n",
        "\n",
        "dim = X.shape[1]\n",
        "\n",
        "Q = tf.placeholder(\"float\", [dim])\n",
        "S = tf.placeholder(\"float\", [None, dim])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmLDDjbhIu-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squared_distance = tf.reduce_sum(tf.pow(Q - S, 2), reduction_indices=1)\n",
        "distance = tf.sqrt(squared_distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEHys2u1Uqsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k = 10\n",
        "\n",
        "top_neg_dists, top_indices = tf.math.top_k(tf.negative(distance), k=top_k)\n",
        "top_dists = tf.negative(top_neg_dists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7doXw3Z-kdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOPb3SRzFqqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "603a2977-e7d0-420f-e7bb-1a466fbfe90c"
      },
      "source": [
        "top_indices.eval({Q:X[0], S:X})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,   37,  273,   69,   33,  241,   75,   26, 1066,   72],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGrb-FahFqwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "daefa956-b753-42ed-e76d-891fb49991ba"
      },
      "source": [
        "np.argsort(euclidean_distances(X[:1], X)[0])[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,   37,  273,   69,   33,  241,   75,   26, 1066,   72])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yDVIZoQCGJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = tf.placeholder(\"float\", [dim])\n",
        "S = tf.placeholder(\"float\", [None, dim])\n",
        "\n",
        "Qr = tf.reshape(Q, (1, -1))\n",
        "\n",
        "PP = tf.keras.backend.batch_dot(S, S, axes=1)\n",
        "QQ = tf.matmul(Qr, tf.transpose(Qr))\n",
        "PQ = tf.matmul(S, tf.transpose(Qr))\n",
        "\n",
        "distance = PP - 2 * PQ + QQ\n",
        "distance = tf.sqrt(tf.reshape(distance, (-1,)))\n",
        "\n",
        "top_neg_dists, top_indices = tf.math.top_k(tf.negative(distance), k=top_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEWw7LOLZdCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3e1af9a0-69b6-48d1-cc7a-bf62f77773f9"
      },
      "source": [
        "top_indices.eval({Q:X[0], S:X})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,   37,  273,   69,   33,  241,   75,   26, 1066,   72],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESN3ZFxw346p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = tf.placeholder(\"float\", [dim])\n",
        "S = tf.placeholder(\"float\", [None, dim])\n",
        "S_norm = tf.placeholder(\"float\", [None, 1])\n",
        "\n",
        "Qr = tf.reshape(Q, (1, -1))\n",
        "\n",
        "PP = S_norm\n",
        "QQ = tf.matmul(Qr, tf.transpose(Qr))\n",
        "PQ = tf.matmul(S, tf.transpose(Qr))\n",
        "\n",
        "distance = PP - 2 * PQ + QQ\n",
        "distance = tf.sqrt(tf.reshape(distance, (-1,)))\n",
        "\n",
        "top_neg_dists, top_indices = tf.math.top_k(tf.negative(distance), k=top_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXEeJHgB4JNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_norm = np.sum(X**2, axis=1).reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPn1RWt58kXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f309047a-56a6-416e-e690-68fa4427e77d"
      },
      "source": [
        "top_indices.eval({Q:X[0], S:X, S_norm: X_norm})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,   37,  273,   69,   33,  241,   75,   26, 1066,   72],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1U4xy5sETxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class L2Retriever:\n",
        "    def __init__(self, dim, top_k=3, use_norm=False, use_gpu=True):\n",
        "        self.dim = dim\n",
        "        self.top_k = top_k\n",
        "        self.use_norm = use_norm\n",
        "        config = tf.ConfigProto(\n",
        "            device_count={'GPU': (1 if use_gpu else 0)}\n",
        "        )\n",
        "        self.session = tf.Session(config=config)\n",
        "        \n",
        "        self.norm = None\n",
        "        self.query = tf.placeholder(\"float\", [self.dim])\n",
        "        self.kbase = tf.placeholder(\"float\", [None, self.dim])\n",
        "        \n",
        "        self.build_graph()\n",
        "\n",
        "    def build_graph(self):\n",
        "      \n",
        "        if self.use_norm:\n",
        "            self.norm = tf.placeholder(\"float\", [None, 1])\n",
        "\n",
        "        distance = dot_l2_distances(self.kbase, self.query, self.norm)\n",
        "        top_neg_dists, top_indices = tf.math.top_k(tf.negative(distance), k=self.top_k)\n",
        "        top_dists = tf.negative(top_neg_dists)\n",
        "\n",
        "        self.top_distances = top_dists\n",
        "        self.top_indices = top_indices\n",
        "\n",
        "    def predict(self, kbase, query, norm=None):\n",
        "        query = np.squeeze(query)\n",
        "        feed_dict = {self.query: query, self.kbase: kbase}\n",
        "        if self.use_norm:\n",
        "          feed_dict[self.norm] = norm\n",
        "        \n",
        "        I, D = self.session.run([self.top_indices, self.top_distances],\n",
        "                                feed_dict=feed_dict)\n",
        "        \n",
        "        return I, D\n",
        "      \n",
        "def dot_l2_distances(kbase, query, norm=None):\n",
        "    query = tf.reshape(query, (1, -1))\n",
        "    \n",
        "    if norm is None:\n",
        "      XX = tf.keras.backend.batch_dot(kbase, kbase, axes=1)\n",
        "    else:\n",
        "      XX = norm\n",
        "    YY = tf.matmul(query, tf.transpose(query))\n",
        "    XY = tf.matmul(kbase, tf.transpose(query))\n",
        "    \n",
        "    distance = XX - 2 * XY + YY\n",
        "    distance = tf.sqrt(tf.reshape(distance, (-1,)))\n",
        "    \n",
        "    return distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AEdGP6OGUM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c59e5d95-8768-44ee-c4a3-58d6a17504be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJIrTVXZdQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/stacko.csv')\n",
        "plot = {}\n",
        "metadata = {}\n",
        "topic_data = {}\n",
        "\n",
        "df = df.dropna()\n",
        "# df.isnull().sum()\n",
        "\n",
        "for id, text in df[['id','text']].values:\n",
        "  plot[id] = text \n",
        "\n",
        "for id, Topics, Tags in df[['id','Topics','Tags']].values:\n",
        "  label = Tags.split()\n",
        "  if len(label):\n",
        "    metadata[id] = {\"name\": Topics,\n",
        "                          \"label\": label}\n",
        "\n",
        "for id in set(plot.keys())&set(metadata.keys()):\n",
        "  topic_data[metadata[id]['name']] = {\"label\": metadata[id]['label'],\n",
        "                                            \"plot\": plot[id]}\n",
        "\n",
        "X, Y, names = [], [], []\n",
        "\n",
        "for topic_name, topic_meta in topic_data.items():\n",
        "  X.append(topic_meta['plot'])\n",
        "  Y.append(topic_meta['label'])\n",
        "  names.append(topic_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7VJ_FjNGF7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_vect=np.load('/content/drive/My Drive/X_vect1.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smgRnX2-Gviw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_vect = bert_vectorizer(X, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4_pN0PICAPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_square_norm = np.sum(X_vect**2, axis=1).reshape((-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Gd4TZEVOJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "550d4be8-967d-4e49-fca6-7be30b25f7e4"
      },
      "source": [
        "test_id = 10060 # The Matrix\n",
        "\n",
        "retriever = L2Retriever(768, use_norm=True, top_k=10, use_gpu=False)\n",
        "%timeit retriever.predict(X_vect, X_vect[test_id], X_square_norm)[0][1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 126 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9AuVbwT-LKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8154ac12-6ef2-4844-d5e0-d19fadf4fcd4"
      },
      "source": [
        "retriever = L2Retriever(768, use_norm=False, top_k=10, use_gpu=True)\n",
        "%timeit retriever.predict(X_vect, X_vect[test_id])[0][1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 329 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCJOkYZB-LNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "822c77f5-23b4-468e-b678-c6bf39a2b4c5"
      },
      "source": [
        "%timeit euclidean_distances(X_vect, X_vect[test_id:test_id+1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 1.45 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHXTVw15N8x1",
        "colab": {}
      },
      "source": [
        "def buildRecommender(topic_names, vectorized_plots, top_k=10):\n",
        "  retriever = L2Retriever(vectorized_plots.shape[1], use_norm=True, top_k=top_k, use_gpu=False)\n",
        "  vectorized_norm = np.sum(vectorized_plots**2, axis=1).reshape((-1,1))\n",
        "  \n",
        "  def recommend(query):\n",
        "    try:\n",
        "      idx = retriever.predict(vectorized_plots, \n",
        "                              vectorized_plots[topic_names.index(query)], \n",
        "                              vectorized_norm)[0][1:]\n",
        "      for i in idx:\n",
        "        print(names[i])\n",
        "    except ValueError:\n",
        "      print(\"{} not found in topic db. Suggestions:\")\n",
        "      # m=8\n",
        "      # n=0\n",
        "      dic={}\n",
        "      q=[k for k in query.lower().split() if k not in ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]]\n",
        "      for i, name in enumerate(topic_names):\n",
        "        l=0\n",
        "        for j in q:\n",
        "          if j in name.lower():\n",
        "            l+=1\n",
        "        dic[i]=l\n",
        "      # for i in sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "      #   print(i[0], names[i[0]])\n",
        "      q=sorted(dic.items(), key=lambda x: x[1], reverse=True)[0]\n",
        "      print(names[q[0]])\n",
        "      recommend(names[q[0]])\n",
        "          \n",
        "  return recommend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1knPUoX-LTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recommend = buildRecommender(names, X_vect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbT-2kOlG-y3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "f00c6296-7105-4bc6-f640-5327858a43e1"
      },
      "source": [
        "recommend(\"how to download a file on Chrome without auto renaming file to “download”?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How to download a file on clicking the name of file using PHP?\n",
            "How to create a ZIP file using PHP and delete it after user downloads it?\n",
            "How to force a file to download in PHP\n",
            "How to change the file name of an uploaded file in Django?\n",
            "How to start file download instantly using php headers\n",
            "How to get full filepath when uploading files in PHP?\n",
            "How to start automatic download of a file in Internet Explorer?\n",
            "Q: How to delete file using file observer in Android?\n",
            "Chrome extension: How to save a file on disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gP-L15jVLU2i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "5e3078a0-a8cd-40e8-e56c-ca2468f1e9a3"
      },
      "source": [
        "recommend(\"download a python code in selenium\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{} not found in topic db. Suggestions:\n",
            "Python: Unable to download with selenium in webpage\n",
            "Use default Chrome profile with Selenium in Python\n",
            "Q: Selenium click does not trigger event on website (python)\n",
            "Chrome headless file download with Selenium in Python\n",
            "Folium map not displaying in Django webpage\n",
            "Q: How can I “click” download button on selenium in python\n",
            "Q: Copy text from website using Selenium for python\n",
            "[python][selenium] on-screen position of element\n",
            "Radio button does not get clicked in Selenium / Python\n",
            "Scraping infinite scrolling website with Selenium in Python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC9QUKWTL2ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f4bc9a22-c814-460e-84ea-402e131fd602"
      },
      "source": [
        "df['Topics'].sample() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51394    Q: weight calculation for panel data set r [du...\n",
              "Name: Topics, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbyKQZF2dMBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save('/content/drive/My Drive/X_vect1',X_vect)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}